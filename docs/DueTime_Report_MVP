Excellent—I'll begin a full deep-dive research and planning report for 'Due Time' based on your clarifications. This will comprehensively cover architecture, design, frameworks, AI integrations, DevOps, UX, and business strategy for an MVP targeted at individual Windows users.

I'll deliver the complete response in structured Markdown with in-depth analysis, tables, and concrete implementation guidance. Please hold on while I compile everything.


# **Due Time: Automated Time Tracking for Windows (MVP Plan)**

## **Objectives and Scope**

**Due Time** is envisioned as a zero-effort time tracking tool for Windows 10+ that **automates** the capture of work hours for individual freelancers, consultants, and knowledge workers. Instead of manual timers, Due Time will run in the background to log application usage, detect idle periods, and intelligently associate activities with projects/clients. Key MVP goals include:

* **100% Automatic Tracking:** Capture active window titles, application names, and usage durations without manual input. Detect idle time (no mouse/keyboard) to pause tracking.
* **Intelligent Categorization:** Use rules and AI to map activities to projects or clients. The user can define mappings (e.g. assign an app or folder to a project) via a friendly UI, while an AI assistant suggests categorizations for new activities.
* **Local-First Data**: All tracking data is stored locally in a SQLite database on the user’s machine for privacy. Provide an option to backup data with encryption. No data is sent to cloud by default (AI features that use cloud APIs will be opt-in).
* **Windows-Native GUI:** A Fluent Design-inspired desktop app for viewing tracked time, editing project assignments, and configuring settings. Aim for an **intuitive UX** with minimal onboarding friction (the app should “just work” in the background, doing 90% of the work automatically and leaving only 10% for user validation).
* **Performance & Footprint:** The tracker service must be lightweight (target <3% CPU and <50 MB RAM usage) so it can run continuously without impacting the user’s workflow.
* **Compliance and Extensibility:** Design with privacy regulations (GDPR/CCPA) in mind – user data stays private and under their control. Ensure the UI is accessible (WCAG-compliant) and the architecture can be extended later for team features, multi-platform support, or integration with other tools.

**Scope of this MVP** is to deliver a functional prototype covering the above essentials, along with foundational work for AI integration (classification and summaries) and a plan for deployment. Stretch goals like team collaboration, mobile apps, advanced analytics, etc., are noted for future consideration but not included in the initial MVP. The following sections provide a detailed breakdown of the system architecture, technology choices, component designs, development process, and go-to-market strategy.

## **System Architecture Overview**

Due Time consists of two primary components: **(1) a background tracking service** that runs continuously to capture usage data, and **(2) a desktop application (GUI)** that allows the user to review and manage their time data. These components interact with a local SQLite database for data storage. The high-level architecture is as follows:

* **Tracking Engine (Windows Service/Agent):** Runs at startup (with user login) and uses Windows APIs to detect the currently active window and application. It logs focus shifts and periods of user inactivity. This engine writes events to the local SQLite database (e.g. “9:00–9:30am: Visual Studio Code – ProjectX repo”). It operates mostly independent of the UI, enabling continuous tracking even if the GUI is closed.
* **Local Database (SQLite3):** Stores all time log entries, project/client definitions, and mapping rules. Chosen for its light footprint and reliability as an embedded database. The DB will reside in the user’s AppData (or chosen directory) and can be encrypted or backed up as needed.
* **Desktop GUI (WPF Application):** A Windows desktop app (following Fluent Design guidelines) that the user can open to see their tracked time, configure project mappings, and view reports or AI-generated summaries. The GUI accesses the SQLite database (read/write) to fetch data and save user settings. It may communicate with the tracking service either via the DB or IPC (Inter-Process Communication) if real-time updates are needed (for MVP, reading from DB periodically is sufficient).
* **Cloud AI Services (Optional, On-Demand):** When AI features are enabled, the app will make secure calls to cloud APIs (e.g. OpenAI GPT) to classify uncategorized entries or to generate weekly summary reports. These calls will be infrequent and only send minimal necessary data (e.g. window titles or aggregated stats) to preserve privacy. All AI results are stored back in the local DB.
* **Backup/Sync Module:** Responsible for backing up the SQLite data file (locally or to a cloud drive) with encryption. In the MVP this may be a simple manual backup button that produces an encrypted file. (Future iterations could automate backups or provide cloud sync across devices.)

**Workflow:** The tracking service logs time segments continuously. The user can open the GUI to review their timeline (e.g. a timeline view of the day or a list of activities) and assign any unclassified segments to the correct project/client. The mapping rules help auto-assign common applications or file paths to projects, reducing manual effort. At the end of the week, the user can generate a summary report (optionally using AI for a natural-language summary). Throughout, the system prioritizes privacy (data stays local) and low overhead.

## **Technology Stack Evaluation**

We evaluated several frameworks for building the Windows desktop application and service, focusing on our requirements: native look-and-feel, performance efficiency, and potential cross-platform expansion. The primary candidates are **.NET 6 WPF**, **.NET MAUI**, and **Electron** (with a web stack). Below is a comparison of these options:

| **Criteria**                   | **.NET 6 WPF (Windows Presentation Foundation)**                                                                                                                             | **.NET MAUI (Multi-platform App UI)**                                                                                                                                                                             | **Electron (Web stack)**                                                                                                                                                                              |
| ------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Target Platform**            | Windows-only (desktop). Mature for Win7–11.                                                                                                                                  | Cross-platform (Windows, macOS, Linux for desktop; plus iOS/Android for mobile).                                                                                                                                  | Cross-platform (Windows, macOS, Linux) via Chromium container.                                                                                                                                        |
| **UI Framework & Design**      | Native Windows UI with XAML. Full access to Win32 and Fluent Design elements. Can achieve a truly native look.                                                               | Abstracted UI (XAML) mapped to each platform’s controls (uses WinUI on Windows). Fluent design on Windows, but on other platforms UI follows their native style.                                                  | HTML/CSS/JS UI rendered in Chromium. Highly flexible styling, but not using native UI controls (must recreate Fluent look in web tech).                                                               |
| **Performance (CPU & Memory)** | High performance, compiled C# code. Low memory overhead (no heavy runtime beyond .NET). Generally faster and more memory-efficient than Electron.                            | Good performance for common scenarios, but slightly higher overhead than WPF on Windows due to cross-platform abstractions. Still compiled .NET, so far more efficient than JavaScript.                           | Noticeably heavier: runs a Chromium browser + Node.js instance. Baseline memory usage \~80–130 MB even for simple apps, and CPU overhead for rendering web UI. JS execution is slower than native C#. |
| **App Size & Footprint**       | Small deployment (the app binary and .NET runtime). XAML UI definitions are compact. Typical installer tens of MB.                                                           | Moderate – single project produces platform-specific binaries. Windows deployment includes .NET runtime, similar order of magnitude to WPF. Mobile targets add overhead (not used in our case).                   | Large bundle: must ship Chromium (\~150 MB) with app. Disk footprint and updates are heavy.                                                                                                           |
| **Development Maturity**       | Very mature (over a decade of usage). Stable, rich tooling (Visual Studio designer, XAML Hot Reload, UI Automation). Lots of community knowledge.                            | Newer (released 2022). Rapidly improving but some desktop-specific features still catching up. Tooling is improving; no drag-drop designer, but XAML Hot Reload exists.                                           | Mature in web context. Huge ecosystem of web libraries. However, desktop integration (notifications, file system, etc.) relies on Electron APIs – less coverage than .NET’s access to Windows.        |
| **Fluent UI Support**          | High – can use WinUI 3 libraries or custom styles to implement Fluent Design (acrylic, reveal, etc.) on Windows 10/11. Direct access to system theming.                      | Moderate – on Windows, it uses WinUI which is inherently Fluent. On other OS, Fluent principles might not apply (uses native iOS/Android controls). For Windows target, nearly on par with WPF in Fluent support. | Low – no built-in Fluent design; would need to mimic through CSS frameworks. Lacks true acrylic blur and other native effects without custom code.                                                    |
| **Native API Access**          | Full .NET access to Windows APIs (via P/Invoke if needed). Ideal for low-level hooks (e.g. tracking user activity).                                                          | Limited to what .NET provides cross-platform. On Windows, can still invoke Win32 APIs, but with caution (to not break cross-platform compatibility).                                                              | Limited – Electron exposes some OS APIs, but not all. Deep Windows integration (like low-level keyboard hooks) is harder or requires native addons.                                                   |
| **Developer Skill Fit**        | Great for C#/.NET developers, especially those familiar with XAML (WPF/UWP). Steeper learning curve for web developers.                                                      | Good for .NET developers; similar XAML approach as WPF. Also allows Blazor (web UI in .NET) which might appeal to web-skilled devs. Requires knowledge of multi-platform nuances.                                 | Great for web developers (uses JavaScript/TypeScript, HTML, CSS). Low learning curve if coming from web background. .NET developers may need to learn web tech.                                       |
| **Cross-Platform Need**        | None (Windows only). Best if we only target Windows desktop (which is our MVP scope).                                                                                        | Yes, if future plans include Mac or mobile, MAUI offers one codebase. However, desktop-specific optimizations might be limited.                                                                                   | Yes, also one codebase for all desktops. But heavy resource usage makes it less ideal for an always-on utility app.                                                                                   |
| **Deployment**                 | Can produce an .exe or MSIX installer easily. Can be distributed via Microsoft Store (packaged as MSIX). No additional dependencies for user (if using self-contained .NET). | Similar to WPF on Windows (MSIX or .exe). For cross-platform, separate builds for each OS. More complex CI to package for each platform.                                                                          | Distribute an installer (built with Electron packagers). Users essentially install a large app. Auto-update is built-in with Electron but increases complexity (updater service).                     |
| **Example Use Cases**          | Windows utilities, enterprise desktop apps, any scenario needing tight Windows integration and performance. (E.g. Visual Studio itself uses WPF for UI.)                     | Apps that target both desktop and mobile or need some native UI on each. Good for companies wanting to share code across platforms.                                                                               | Apps where rapid development and web-like UI are priorities, and resource usage is secondary (e.g. Slack, VS Code). Not typical for background services due to overhead.                              |

**Decision:** For this MVP, **.NET 6+ WPF** is the recommended choice. We are targeting Windows desktop exclusively (no immediate need for cross-platform), and performance is a top priority. WPF allows direct use of Windows APIs for tracking, has a mature ecosystem, and integrates well with Fluent Design on Windows. By using WPF on .NET 6/7, we get modern .NET benefits (performance, memory management) while keeping the app lightweight and native. .NET MAUI is powerful for cross-platform goals, but since we don’t need mobile support in the MVP and MAUI’s desktop support is still evolving, it would introduce unnecessary complexity. Electron is not suitable due to its high memory and CPU overhead for an always-running background app; even a minimal Electron app can consume over 100 MB RAM and significant CPU, which clashes with our performance targets. In short, WPF best meets our needs for a **fast, low-resource Windows-native** application.

*(For completeness, alternative .NET technologies like **WinUI 3 (Project Reunion)** or third-party frameworks like **AvaloniaUI** were also considered. WinUI 3 could deliver a modern Fluent UI, but it is primarily used via UWP/Packaged apps or via MAUI and is still maturing. AvaloniaUI is a cross-platform XAML-based framework that some developers prefer over MAUI for desktop apps; it could be a future consideration if we port to Linux/macOS. However, given our Windows focus and the team’s familiarity with WPF, we’ll proceed with WPF for the MVP.)*

## **Detailed Component Design**

### **1. Tracking Engine (Background Service)**

The tracking engine is the heart of Due Time, running as a background Windows service or startup application to continuously log user activity. Its design emphasizes efficiency and accuracy:

* **Active Window Monitoring:** The engine uses the Windows API to detect when the foreground (active) window changes. Rather than polling rapidly (which wastes CPU), we will use an event-driven approach via `SetWinEventHook` to subscribe to the `EVENT_SYSTEM_FOREGROUND` event. This OS hook notifies our callback whenever the user switches to a different window. Upon receiving this event, the service will retrieve the new foreground window’s handle and query its title and process name (e.g., via `GetForegroundWindow` and `GetWindowText` or .NET’s `Process` API). Each time focus changes, we can record an “end time” for the previous activity and a “start time” for the new one. This granular approach ensures we capture app switches promptly without constant polling.

* **Idle Time Detection:** To avoid counting away-from-keyboard time as work, the engine will detect user idle periods. We will call the Win32 function `GetLastInputInfo` periodically (e.g., every 5 seconds) to check the timestamp of the last user input (keyboard or mouse). If the difference exceeds a threshold (configurable, e.g., 2 minutes), the engine will mark the user as “idle” and stop incrementing the active task’s timer until input resumes. An idle segment can be logged if it’s significant (e.g., “Idle from 10:30–10:45”). This is similar to how other trackers handle afk time – for instance, ManicTime’s tracker detects when the user “step\[s] away” and can later prompt tagging of that idle period. Our engine will log idle events so that the UI can later present them for the user to categorize or discard (e.g., mark as “Break”).

* **Application & Document Capture:** In addition to the application name, capturing context like the window title or open document path is crucial for intelligent tracking. The engine will attempt to log the **window title** of the foreground app (many time trackers do this to provide more detail – ManicTime even logs document names/URLs). For example, if the user is in Microsoft Word with a document named “ClientA\_Proposal.docx”, the window title provides that context. Similarly, a browser’s title can indicate the website. We will store these titles, and in some cases the path (for apps where we can get the open file path via automation or by window title parsing). This data will feed into the categorization logic (rules could match keywords in titles).

* **Associating with Projects:** The tracking engine itself does not definitively know project assignments (that logic lives in the mapping rules and AI). However, it can **auto-apply known rules** at log time to tag entries with a project if a rule matches. For example, if the user previously mapped “Visual Studio Code with folder path D:\ProjectX” to “Project Alpha”, the engine can detect when VS Code’s title/path contains “ProjectX” and automatically tag that time chunk as Project Alpha. Initially, simple heuristics (exact match or substring in window title, or matching process name to an application tied to a project) will be used. Unmatched entries get a null project (to be assigned later in the UI).

* **Data Buffering and Write Strategy:** To minimize disk I/O overhead (and prevent frequent DB writes), the service can buffer the current activity in memory and only write to SQLite when an activity ends (i.e., on window change or when idle begins). Each log entry will include: start\_time, end\_time, application\_name, window\_title (and perhaps file\_path or URL if available), an “idle” flag or activity type, and an associated project\_id (if known). We will ensure that writes are batched or at least not happening more often than, say, a few per minute in typical use. This helps keep CPU and disk usage low.

* **Service Implementation:** We plan to implement this as a Windows Tray application that starts on login (and can be minimized to tray) **or** as a Windows Service running in the background. A user-level service has the advantage of running even if the user closes the GUI. Many similar tools (RescueTime, ManicTime) use a separate background process/service. For MVP, a simpler route is to run the tracking engine in the same process as the tray GUI app (so one exe handles both), and ensure closing the window only minimizes to tray rather than fully exits. This avoids IPC complexity. However, we will design the code such that the tracking logic is separate from UI code, making it easy to refactor into a service later.

* **Performance Optimizations:** Using the event hook for window changes ensures near-zero CPU when the user isn’t switching apps. The idle check timer (every few seconds) is lightweight. We will avoid expensive operations on each event – e.g., do string pattern checks for rules in a performant way (pre-compiled regex or simple substring matching) and possibly offload AI classification to a background thread or queue so it doesn’t block the capture loop. The target is that the tracking engine’s CPU usage stays negligible (observing tools like RescueTime and ManicTime shows their background processes usually stay below 1-2% CPU). Memory-wise, the engine is just keeping some strings and timestamps – well within tens of MB. By using .NET 6’s improved garbage collector and spans where appropriate, we ensure minimal memory churn.

* **Example:** Suppose the user works from 9:00–9:30 in Chrome (on `docs.google.com`), then at 9:30 switches to Excel, and goes idle at 10:00. The engine would log something like: (9:00–9:30, “Google Chrome”, title “Project Plan – Google Docs”, no project assigned yet) and (9:30–10:00, “Microsoft Excel”, title “Budget.xlsx - Excel”, project “ClientX” if a rule matches “Budget.xlsx” to ClientX). When idle starts at 10:00, it closes the Excel entry at 10:00. If the user returns at 10:15, an “Idle from 10:00–10:15” entry is recorded. This data is now ready for the user to view and confirm categories in the GUI.

### **2. Desktop GUI – Fluent Design & Mapping Tool**

The desktop application provides the user interface for Due Time, allowing users to review their tracked time and configure the system. We will build it with **WPF (C# and XAML)**, following Fluent Design principles to blend with Windows 10/11 aesthetics:

* **UI Structure:** The GUI will have a main dashboard screen and a few focused dialogs or pages:

  * **Dashboard View:** Shows a summary of today’s (or selected period’s) activity. This might include a timeline visualization (stacked bars for each project on a time axis) and/or a list/table of entries. For MVP, a tabular “daily timeline” with time slots and project/application info is simpler to implement. For example, a list of entries: 9:00–9:30 Chrome (Unassigned), 9:30–10:00 Excel (ClientX), 10:00–10:15 Idle, 10:15–11:00 VS Code (Project Alpha), etc., possibly grouped by project or by hour. Users can click on an entry to edit its project assignment. RescueTime’s UI, for instance, presents a timeline with highlights and allows drag-drop editing, but we might defer drag-drop and use context menus or edit dialogs for simplicity.
  * **Project/Client Management:** An interface to create and edit Projects and Clients. Each project can have attributes like name, client (link to a client entry), color (for display), and **“classification hints”** – keywords or rules that help identify it (e.g., “files containing ClientName, or Window title contains XYZ”). These hints are similar to RescueTime’s “AI Classification Hints” which improve automatic assignment. The user will be able to define a mapping rule in a simple way, such as “Any window title containing ‘ClientX’ or file path containing ‘ProjectXFolder’ → Project X”.
  * **Mapping Tool (Click-to-Capture):** We plan a convenient way for the user to create a rule by capturing the context of an open window. For example, a button “Assign current window to project…” which when clicked, grabs the active window’s title and application, then prompts the user to pick which project this should always map to. This can automatically create a rule (perhaps storing the exact window title or a fuzzy match pattern). Alternatively, the user could drag an eyedropper tool onto a window to capture its title (similar to how color-picker tools work, but here capturing window identity). For MVP, a simpler approach: provide a dropdown in the UI listing recent uncategorized applications/titles; the user can select one and assign it to a project, which creates a persistent rule.
  * **Reports View:** Beyond raw data, the app will generate some useful reports. Initially, a **weekly summary** screen that totals hours per project and perhaps shows a trend. We will also have a button to “Generate Weekly Summary (AI)” which calls the AI to produce a paragraph of insights (discussed in AI section). This view will help users see the bigger picture and is a selling point (similar to RescueTime’s reports on productivity or Clockify’s timesheet view).
  * **Settings:** Options like idle timeout duration, whether to auto-run on startup, theme (light/dark – we will respect Windows theme by default), and toggles for features (e.g., “enable AI suggestions”, “enable screenshot capture” (stretch), etc.). Also a place to manage backup/restore.

* **Fluent UI Design:** We will adhere to Microsoft’s Fluent Design guidelines to make the UI modern and familiar. This includes using **acrylic/translucent** backgrounds for panels if possible (WPF can implement acrylic blur using the Windows 10 AcrylicBrush API or custom code), **reveal highlights** on interactive elements, **segmented controls and typography** consistent with Windows 11. The design principles of Fluent (“Effortless”, “Calm”, “Personal”, “Familiar”) will guide our UX – meaning the app should feel native, not distracting or jarring. For instance, the color palette will follow system accent colors; we’ll support dark mode and light mode. The layout should be **calm** and uncluttered, showing key information at a glance (total hours, etc.) without overwhelming charts unless requested. We aim for **effortless** interactions: e.g., editing a project assignment is just a right-click or inline dropdown, not a complicated dialog sequence.

* **Usability Considerations:** Since the app runs continuously, we’ll implement it as a tray icon with context menu. Clicking the tray icon opens the dashboard. The user can also right-click tray to quickly do things like “Pause tracking for 30 minutes” (in case they want to temporarily stop logging) or “Open Due Time”. This is similar to how other time trackers and many background apps behave. The **onboarding** for the UI will be minimal: when first launched, it might show a one-time tooltip tour (pointing out “Here’s where you see your time, here’s how to assign projects…”). However, we expect the user to learn by doing – since by default, the tool is already tracking, the first thing a user might do is go into the app and assign any “Unassigned” time to projects. We will ensure the UI makes this easy (perhaps an “Unassigned” filter that lists all unassigned entries with checkboxes to assign them in bulk to a selected project).

* **Data Editing & Validation:** The user can retroactively adjust any logged time segment. For example, if a window was mis-classified or left unassigned, the user can change its project. This will update that record in the DB (and possibly also create a new rule if the user chooses “always assign similar entries to this project”). The app should also handle splitting or merging entries – e.g., if two consecutive entries belong really to one task, or if a single long entry needs to be split. Some advanced editing might be stretch, but at least basic reassign and deletion (e.g., remove an erroneous entry) will be supported.

* **Accessibility:** Using WPF and proper XAML practices, we get a lot of accessibility support out of the box (WPF supports UI Automation for screen readers, keyboard navigation, etc. by default). We will label controls, use sufficient color contrast (especially in charts or timeline – ensure the color choices for projects are distinguishable in grayscale or for color-blind users). The Fluent Design’s emphasis on **personal and familiar** usage also means we consider users with different needs: for instance, we’ll support high-contrast mode (WPF can pick up system high-contrast settings) and ensure that the app is usable entirely via keyboard (important for power users too). All functionalities (like assigning a project to an entry) will be accessible via keyboard shortcuts or focus + enter, not just mouse.

* **Example Interaction:** The user opens Due Time’s GUI after working in the morning. They see a **timeline** for today – a colored bar shows green from 9–10am (Project Alpha), blue from 10–11am (Project Beta), etc., with grey portions indicating idle time. Below, a list of entries shows details: “09:00–10:00 Photoshop \[Project Alpha]”, “10:00–10:15 Idle \[Unassigned]”, “10:15–11:00 Google Meet \[Project Beta]”, etc. One entry is “Idle” and unassigned – the user can double-click it and mark it as “Personal” or “Break” (perhaps creating a special category rather than a client project). The interface uses icons (e.g., a coffee cup icon for idle/break suggestion) to hint common actions. The user then switches to the **Projects** tab and adds a new project for a client they will start working with, adding a hint “ClientCo” as a keyword. Later, when they work on something containing “ClientCo” in the window title, the system will auto-tag it. The UI contributes to this seamless experience by making configuration simple and by presenting the tracked data in a clear manner.

### **3. Data Storage, Schema, and Backup**

All application data will be stored locally using **SQLite 3** as the database engine. SQLite is lightweight, serverless, and proven in numerous applications (and can handle hundreds of thousands of records efficiently, which is important as we accumulate time logs). Key aspects of our data design:

* **Schema Design:** We will create a normalized schema to store time entries and related entities. The tentative schema includes:

  * **Project** table: `Project(id, name, client_id (nullable), color, hints (text))`. Each project can be associated with a client. `hints` may store comma-separated keywords or patterns relevant to the project (for AI and rule-based matching).
  * **Client** table: `Client(id, name)`. A simple list of client names for grouping projects (could also be just a field in Project, but separate table allows reuse of client info if we extend in future).
  * **TimeEntry** table: `TimeEntry(id, start_time, end_time, app_name, window_title, project_id (nullable), is_idle (boolean))`. This table will be the largest, logging each contiguous period of activity. If `project_id` is null, it means unassigned (to be classified). If `is_idle` is true, the entry represents idle time (and likely no project\_id). We also include perhaps `duration` (but can be computed from start/end) or store end\_time as null for an entry in progress.
  * **Rule** table (optional for complex mappings): `Rule(id, project_id, app_name_pattern, title_pattern, path_pattern)`. This could store user-defined mapping rules. For MVP, we might not need a separate table (could incorporate in Project.hints or just apply rules from project data), but a table allows multiple rules per project or more complex logic (e.g., one project might have several keywords). Initially, the UI-driven mapping creation can populate this. The tracking engine will read these rules at startup to apply auto-tagging.
  * **Settings** table: key-value pairs for various user settings (including last backup date, preferences toggles, etc.).

  The schema will evolve with features; we’ll ensure it’s easy to migrate (since SQLite supports ALTERs or we can do simple versioning).

* **Data Volume & Indexing:** The time entry table will grow with every tracked period. Assuming a user works \~8 hours a day and switches tasks perhaps 50 times a day, that’s \~50 entries/day. In a year, \~18k entries, which SQLite can handle easily (especially with proper indexing). We will index at least on `start_time` (to query ranges by date) and possibly on `project_id` (to get totals per project quickly). The app’s typical queries: “fetch all entries for today” or “for this week” (range scan by time), and “fetch all entries for project X in period Y” (which uses project\_id index). These queries should be fast with SQLite given the scale.

* **Security & Encryption:** By default, SQLite databases are not encrypted. Since our app deals with potentially sensitive data (e.g. filenames might reveal client names, etc.), we plan to offer **encrypted backup** at minimum. For the MVP, the approach is:

  * **Encrypted Backups:** When the user triggers a backup (or at scheduled times), we will export the SQLite DB to a file and encrypt that file (for example, using 256-bit AES encryption with a user-supplied passphrase). This could be implemented via a .NET library (like `System.Security.Cryptography` AES) to create an encrypted .zip or a custom encrypted file. The user would then have a .backup file that is safe to store on cloud storage. On restore, they enter the passphrase to decrypt and replace the DB.
  * **Full DB Encryption (optional):** As a stretch, we consider using **SQLCipher**, an open-source extension that encrypts SQLite databases transparently. SQLCipher is widely used in mobile apps and ensures the database file at rest is encrypted with 256-bit AES. Integrating SQLCipher in .NET might require a third-party driver. If time permits, we could incorporate it so that the database is always encrypted (user would need to enter a password on app launch to decrypt). If not, we clearly document that the DB is plaintext on disk and rely on OS security (user account access) plus the encrypted backups for security. Given privacy is a priority, using SQLCipher would be ideal to become “the de facto standard for encrypted local databases” in our application.

* **Backup & Restore UI:** In the GUI Settings, we’ll have a “Backup Data” option. This lets the user choose a location to save an encrypted backup of their data. The backup process will pause the tracking (to avoid writes), sync any pending data to DB, then copy out the DB, encrypt it, and resume tracking. Similarly, a “Restore” will require the user to pick a backup file and enter the password, then replace the current DB (likely followed by an app restart to ensure in-memory data matches). We’ll warn the user to close the app or pause tracking when restoring to avoid conflicts. In future, we might automate backups (e.g., weekly backup to a chosen folder).

* **Cloud Sync (Future):** For MVP, no cloud sync of live data is planned (to maintain local-first stance). However, we structure the data layer such that introducing sync later (to sync between multiple devices or to a team server) is feasible. That might involve identifying each record by a GUID and tracking modifications for upload. As a stretch concept, we can consider using something like an online storage integration (e.g., allow the SQLite DB or backup to be placed in a user’s OneDrive/Google Drive so that it’s at least backed up by their cloud). Some users might do this manually by pointing the data directory to a synced folder. We will ensure the app can handle that (SQLite can work on network drives or synced drives but with caution on multi-access – we assume single user access at a time).

* **Data Privacy:** By keeping the data local and giving the user control over backups, we align with privacy best practices. The user can inspect the data (perhaps we’ll provide an “Export to CSV” for their records) and delete it whenever they want (uninstalling the app can offer to delete the data file). This helps with GDPR compliance – if the user wants to exercise data deletion, it’s entirely under their control (no server copy exists). We’ll include documentation that explains where the data is stored and how to wipe it if needed.

### **4. Privacy, Security & Compliance**

From the outset, Due Time is designed with a **privacy-first philosophy**: the data collected (which apps you use, for how long) is personal and sensitive, so it should remain under the user’s control. Key measures and compliance considerations:

* **Local Data & User Control:** All tracking data is stored locally on the user’s machine, not sent to our servers. This is similar to how Clockify’s Auto Tracker operates (keeping data local and not syncing it automatically). Only the user can view the full detailed activity logs. We explicitly distinguish Due Time from “employee monitoring” tools – much like RescueTime emphasizes it “is not employee monitoring” and that it respects privacy. In our context, that means we are building this for the individual user’s benefit, and no one else (including an employer or us as developers) has automatic access to the data. If in future team features are added, they will be opt-in and likely share only aggregate data with consent.

* **GDPR/CCPA Compliance:** Although we are not processing data on our servers, we still follow principles of these regulations:

  * We minimize data collection to only what’s necessary for functionality (window titles, app names, timestamps). We avoid capturing any keystrokes or actual content within documents (so no keylogging or screenshotting in MVP, which could capture personal data). We also allow the user to **export or delete** their data easily (for GDPR “right of access” and “right to be forgotten”). For example, an export to CSV or JSON of all time entries by date will be provided. Deletion can be achieved by deleting the database via the app’s UI (perhaps a “Reset Data” button).
  * If the user enables cloud AI features, we will clearly inform them that some data (like an app name or summary of activity) will be sent to a third-party AI service (OpenAI). This is important for transparency. We will provide a toggle to enable/disable these features, and if disabled, no external data transfer occurs.
  * We will have a privacy policy (likely in documentation or website) explaining exactly what data is collected and how it’s used solely locally. Although not required for a local-only app by law, it’s good practice.

* **Security:** Running a background service means we should consider its privileges. We will run the service under the user account (not as SYSTEM) to limit scope. The data on disk can be encrypted (as discussed via SQLCipher or backups). Additionally, we’ll protect the data in memory by not unnecessarily storing sensitive info longer than needed. One risk to consider is if the machine is multi-user – by default, another admin user could potentially access the SQLite file. Encryption mitigates that. We can also allow the user to set an app password that encrypts the database (this is essentially what SQLCipher would do). For MVP, we might trust OS-level security and revisit this if user demand is high.

* **Compliance: Accessibility (WCAG):** We ensure the app meets accessibility guidelines (WCAG 2.1 AA for contrast, etc.). Using standard WPF controls means screen readers can navigate our UI. We will test with Windows Narrator for basic screens. All interactive elements will have tooltips or labels. We will also make sure that color is not the sole indicator of information (e.g., the timeline chart will use patterns or labels so that colorblind users can interpret it – or at least allow toggling to a text view). These efforts ensure the app is usable by people with disabilities, aligning with both ethical design and certain legal requirements (like ADA in the US).

* **Data Retention Policy:** By default, we do not delete any user data (the user can have all historical data as a personal archive). However, we might implement an **auto-cleanup option** where the user can choose to purge data older than X months. Clockify’s auto-tracker, for instance, only keeps 45 days of data locally by design – in our case, because data is local and storage is usually not an issue (text data for years will only be a few MBs), we prefer to keep it indefinitely. But a user might ask for a rolling deletion for privacy. We’ll make this configurable.

* **Third-Party Compliance:** If we integrate any third-party libraries (like SQLCipher or AI SDKs), we will ensure they don’t silently transmit data. OpenAI usage will be direct via API calls with only the data we send in prompts. We should also comply with OpenAI’s policies (e.g., not sending truly sensitive personal data without anonymization). For example, if window titles contain person names or emails, that could be personal data – we might consider an option to mask certain data before sending to AI (perhaps an advanced feature where user can set “never send anything containing XYZ to cloud”). In MVP, we’ll rely on user judgment (they opt in to AI and are informed of the risk that window titles might contain sensitive info).

In summary, privacy and security are at the core of Due Time’s design: the user’s data stays theirs. We combine local-first storage, encryption for backups, and transparent user controls to fulfill that promise. This not only addresses GDPR/CCPA legal points but also is a **market differentiator** for users who are concerned about the privacy of tools like RescueTime (cloud-based). We want users to feel confident that **“everything tracked is private for your eyes only”**.

### **5. AI Integration (Auto-categorization and Summaries)**

A standout feature of Due Time will be its AI-powered capabilities to reduce manual work even further. We plan two AI-driven functions: **automatic activity classification** and **weekly summary generation**. These will leverage cloud AI (such as OpenAI’s GPT-4 or GPT-3.5 models) through their API. All AI features will be optional and require user opt-in (and possibly an API key or premium account if we make it a paid feature). Here’s the design and plan for these features:

* **Automated Categorization (AI Assistant for Project Assignment):**
  The idea is to have the AI suggest or even automatically assign the project for a given time entry if the system is not sure. This is similar to RescueTime’s new “History & AI” timesheet assistant, which uses OpenAI to guess which project you were working on. Our implementation plan:

  * When a new TimeEntry is recorded (or when reviewing uncategorized entries), we gather context about that entry: application name, window title, perhaps the file path or URL (if available), time of day, and potentially the user’s recent activity sequence. We also have the list of defined projects and their “hints” (keywords the user provided).
  * If an entry remains **Unassigned** and the user has AI enabled, we will call an AI classification function. This could be done in batch (e.g., at the end of day or when the user opens the app) for all unassigned entries, or in real-time as entries come in. Batch is more efficient (one API call can handle multiple entries if using a function like GPT’s ability to output a JSON classification for each).
  * **Prompt engineering:** We will craft a prompt for the AI like: *“The user has the following projects: Project Alpha (keywords: AlphaCorp, OracleDB), Project Beta (keywords: Beta Inc, UX Design), ... You see an activity: Application=Chrome, Title='Beta Inc – Home - Jira'. Suggest which project this belongs to, or 'Uncertain' if unclear.”* The AI would likely respond “Project Beta” given the keyword. For multiple entries, we might feed a list and ask for a list of classifications. If using an OpenAI function calling or fine-tuning approach, we could structure it more formally.
  * **AI Confidence and Confirmation:** The AI suggestions will be presented to the user for confirmation rather than automatically applied (at least in MVP). For instance, the UI might highlight an unassigned entry with a suggested label “(Suggested: Project Beta)” which the user can accept or override. This human-in-the-loop approach ensures accuracy and builds trust (the user sees the AI is helping, but they remain in control). Over time, if suggestions are consistently good, we could allow an “auto-approve high confidence suggestions” setting.
  * **Implementation Details:** We’ll likely use the OpenAI API (text completion or chat models). GPT-3.5 should be sufficient for classification with proper hints, which keeps costs low. Alternatively, we might use a smaller model or ML.NET local model for straightforward cases (like a Bayes classifier on window titles). However, the advantage of GPT is that it can use general knowledge (e.g., knowing that “photoshop.exe” usage likely is design work) and user-specific context. We should be mindful to not send large amounts of data: we won’t, for example, send the entire day’s log text. Only the relevant strings and project names. Also, multiple entries can be combined in one prompt to amortize costs.
  * **Learning from User Input:** Over time, as the user corrects AI or assigns projects manually, we can feed that as context to future prompts or even fine-tune a custom model. MVP will stick to prompt-based zero-shot or few-shot classification. We might include recent examples in the prompt (“Yesterday, ‘SalesForecast.xlsx’ was categorized as Project Alpha”) to give the model context.

  *Example:* User has Project Alpha (for client Alpha Corp) and Project Beta. They work on an Excel file “AlphaCorp\_SalesForecast.xlsx” for an hour, but had not created a specific rule for it. The AI sees: Excel with title containing “AlphaCorp”. It recognizes “AlphaCorp” likely relates to Project Alpha (based on the project name or hint). It suggests Project Alpha. The UI marks that entry with a tag or color indicating an AI suggestion. The user sees it and clicks “Accept”. The entry is now assigned, and perhaps the system can optionally turn that into a new rule (so next time any Excel with “AlphaCorp” in the title auto-assigns to Project Alpha without even needing AI). This way AI and deterministic rules complement each other.

* **Weekly/Daily Summary Generation (AI Reports):**
  In addition to raw numbers, it can be incredibly useful (and engaging) to get a narrative summary of your time. We’ll use an AI (GPT model) to generate a **weekly report** for the user. This is akin to having a personal assistant write an overview of what you accomplished, where time went, etc. The design:

  * At the end of the week (or whenever the user requests, e.g., “Generate Summary” button), the app will compile key statistics: total tracked hours, breakdown by project (e.g., Project Alpha: 12h, Project Beta: 8h, 4h uncategorized or idle), notable applications used (maybe top 3 apps by time), and any outliers (longest workday, etc.). We then feed these stats to the AI and ask it to produce a friendly summary. For example: *“Summarize the work week: Project Alpha (12h) focused on design and documentation, Project Beta (8h) mostly meetings, 4h were breaks or idle. Highlight any interesting patterns.”* The AI might return: *“This week, you spent about 20 hours on work. The majority (12 hours) went into Project Alpha, where you worked extensively on design tasks and documentation. Project Beta accounted for 8 hours, including several meetings and planning sessions. You took approximately 4 hours of break time. Notably, Tuesday was your most productive day with 7 hours of work. Keep an eye on the 4 hours of unassigned time – perhaps some of it can be allocated to projects to ensure all work is accounted for.”* This provides insight in natural language.
  * We could also generate **daily summaries** or even allow the user to ask questions (“When did I spend time on Project Beta this week?”). But MVP will focus on a structured weekly recap.
  * **Technical:** This is a straightforward use of GPT-3.5/4 for summarization. We will template the prompt to ensure important data is included and the tone is appropriate (professional yet accessible). Since the output is purely for the user, we can allow some creative flourish as long as facts are correct. The cost of one summary call per week is minimal, so this is feasible even if we have many users (if we cover the cost under a subscription, it’s part of monetization considerations). Alternatively, we could have the user supply an API key for OpenAI in settings, which shifts cost to them but some users may not like that complexity. We’ll decide based on monetization model (discussed later).
  * The summary text will be shown in the Reports section. We might also allow exporting it or emailing it. For freelancers, such a summary might even be something they share with a client to report progress (though carefully – they might edit it).

* **AI Tooling & Development:** During development, we will test these features with our own data (possibly by simulating usage logs) to refine prompts. We might use a smaller model for local testing (like GPT-2 or open-source models via transformers) just to validate logic without cost, but the quality difference is large, so final testing will be with OpenAI’s models. We should also keep an eye on Microsoft’s offerings (like Azure AI or Copilot Labs) in case they have suitable APIs, but OpenAI is the most direct route.
  In the code, we’ll isolate AI calls behind an interface so we could switch providers if needed. For instance, use a service class `AIClient` with methods `ClassifyEntries(entries)` and `SummarizeWeek(data)` that currently call OpenAI. If later we fine-tune our own model or switch to Azure, we change that internally.

* **Ethical AI & Privacy:** We will instruct the AI not to expose any sensitive info back that wasn’t provided. Since it’s only working with the user’s data anyway, the risk is low. We also should handle cases where the AI might be unsure – it could say “Uncertain” and we handle that gracefully by leaving it unassigned. If AI is unreachable (network down or API error), the app will simply not provide suggestions, ensuring it fails safely (the core functionality doesn’t depend on AI).

* **Comparison to Competitors:** Our approach is in line with industry trends. RescueTime has started integrating OpenAI for project tagging. A newer competitor, Clockk, positions itself as “AI powered (automatic) time tracking” focusing on separating different projects even within the same app – which is exactly what our combination of rules + AI aims to do. We differentiate by keeping the data local and letting AI assist rather than taking over. Over time, our AI model could even learn the user’s patterns specifically if we invest in on-device machine learning (stretch goal: e.g., a local neural network that gradually adapts to the user, which would address any privacy concerns with cloud AI).

**Development Plan for AI Features:** We will scaffold these features in phases:

* *Phase 1:* Define the data structures for AI suggestions (e.g., a field or parallel table to store suggested project for an entry and whether it’s confirmed). Build the UI elements to display suggestions and accept/reject them. Initially, this can be populated by a simple heuristic (like keyword matching) to test the flow.
* *Phase 2:* Integrate OpenAI API for classification. Use a small set of test prompts with sample data to validate that GPT can map correctly given our prompt format. This will be an iterative prompt engineering task. Once satisfactory, implement the logic to batch unassigned entries and send for classification periodically (perhaps when user opens the app or clicks a “Suggest” button to control timing).
* *Phase 3:* Integrate summary generation. This is more straightforward – implement the data aggregation, craft the prompt, get the completion and display it nicely (maybe as a read-only text with copy button).
* *Phase 4:* Testing with real scenarios: we will test with 2-3 different user profiles (e.g., one software developer scenario, one designer, one consultant) and see if the suggestions make sense and if the summary feels useful. Adjust accordingly.
* *Phase 5:* API usage monitoring – we’ll ensure proper error handling (if API quota exceeded or no internet, etc., the app should handle gracefully). Possibly add caching so we don’t call AI on the exact same entry repeatedly (store the suggestion once per entry).

This phased implementation ensures AI features are built on a stable core. Users not interested in AI can ignore/disable it and use the app fully manually (which still provides full functionality).

### **6. DevOps: CI/CD and Release Management**

To deliver Due Time reliably, we will set up a robust DevOps pipeline using **GitHub Actions** for Continuous Integration and Deployment (CI/CD). The development process will be test-driven where possible, and every commit will be validated through automation. Key aspects of our DevOps strategy:

* **Repository Structure:** We’ll host the code on GitHub. The repository might be structured with a main solution containing the WPF app (GUI) and perhaps a Core library (for tracking and data logic) and any service components. We’ll also include test projects (for unit tests of the core logic).

* **Continuous Integration (CI):** On each push or pull request, GitHub Actions will trigger a build and test workflow:

  * **Build Matrix:** Since our app is primarily Windows, we will use the Windows runner for building the WPF project (using `dotnet build` with the appropriate SDK, likely .NET 7 by the time of development). We can also include a Linux runner for any cross-platform libraries or just to run simpler tests, but the main artifact build needs Windows (for WPF).
  * **Unit Tests:** We will run unit tests (e.g., `dotnet test`) on at least one runner (Windows). Core logic like time calculation, idle detection logic (simulated), and database operations can be unit tested. UI code will be minimal in tests, but we might test ViewModels if using MVVM.
  * **Linting/Code Analysis:** We can integrate Roslyn analyzers or StyleCop for code quality, and run those in CI to catch issues (like CA warnings, FxCop, etc.). This ensures maintainable code as the project grows.
  * **Build Artifacts:** The CI will produce binaries – possibly a portable .exe or an installer. For MVP, we might distribute as a simple zipped .exe or an MSI installer created via a tool like WiX or using MSIX packaging. We will configure the workflow to output these artifacts (so testers can download a build from GitHub).

* **Continuous Delivery (CD):** Once we reach a stable release candidate, the pipeline can also handle publishing:

  * We can have a workflow for creating releases. For example, when we push a tag like `v1.0.0`, GitHub Actions can build the release binaries and attach them to a GitHub Release. We’ll include release notes (possibly auto-generated from commit messages or manually written).
  * If we decide to distribute via the Microsoft Store, we would package the app as MSIX and go through the Store submission process. Setting up CI for store submissions is possible (with Store CLI or manually uploading), but likely not for MVP.
  * We will set up code signing for releases. This is important on Windows to avoid SmartScreen warnings. We’ll obtain a code signing certificate and use signtool in the CI (securely injecting the cert) to sign the installer and executable. GitHub Actions secrets store will be used to keep the certificate and password secure.

* **Cross-Platform Builds:** If we had chosen MAUI or needed Mac/Linux builds, we’d integrate Mac and Linux runners accordingly. Since we focus on Windows WPF, we’ll primarily use the Windows runner (which runs on a Windows Server with necessary SDKs). However, for the sake of verifying cross-platform core logic, we might run our core library tests on Ubuntu runner too (this ensures, for example, that our SQLite data access code or any general logic has no platform-specific bugs). This also keeps the door open to a possible Linux version if using something like Avalonia – though not in scope now, testing core logic on multiple OS can highlight any assumptions (like path handling differences).

* **DevOps for AI integration:** We might have separate pipelines or steps for the AI components. For example, we’ll not run AI calls in CI tests (to avoid external calls and cost during tests). We will mock the AI responses in tests. But we might have a nightly build that runs a full integration test with AI (if we have a test OpenAI key) to ensure that part is working.

* **Deployment Environment:** For internal testing, we’ll use a group of Windows 10 and Windows 11 machines to install the app and ensure it behaves (especially the service aspect and auto-run). We can automate some UI testing using tools like WinAppDriver or Appium for Windows if time permits, to simulate a user clicking around. These could be integrated in CI as well (spinning up a VM to run UI tests). However, UI tests can be fragile, so we will likely rely on manual testing for the UI and keep CI automated tests for non-UI logic.

* **Version Control and Branching:** We will use a standard Git branching strategy: e.g., a `main` (or `dev`) branch for ongoing development, feature branches for major features, and a `release` branch or tags for stable versions. Pull Requests require CI to pass before merging. This ensures that nothing broken gets into main. We can also protect main with required code reviews.

* **Issue Tracking and Project Management:** Alongside CI, we’ll use GitHub Issues to track tasks/bugs and possibly GitHub Projects or a similar Kanban to plan sprints (especially since we have a multi-phase plan). Each phase’s key tasks (as defined in the roadmap table later) can be issues or epics.

* **DevOps for Delivery to Users:** For distributing the MVP to pilot users, aside from GitHub releases, we might set up an auto-update mechanism. This could be as simple as the app checking a JSON feed on our website/GitHub for latest version and prompting the user to download. Electron has built-ins for that, but in .NET we could integrate something like Squirrel or just instruct users to manually check. For MVP and pilot, manual or email notification of new builds might suffice. Later, an in-app updater can be added for convenience.

* **Example CI Pipeline Outline:**

  1. **Trigger:** On push or PR to main, run CI.
  2. **Setup .NET** (use actions/setup-dotnet to get .NET 7).
  3. **Build**: `dotnet restore`, `dotnet build -c Release`.
  4. **Test**: `dotnet test` (with coverage perhaps).
  5. **Package**: `dotnet publish -c Release -r win-x64` to get a single-folder or single-file exe. Or run WiX to create MSI.
  6. **Archive artifacts**: Save the build artifacts (exe/installer) for download.
  7. **(On Release Tag)**: Similar to above, then sign the binaries, create a GitHub Release via API, attach files. Notify the team.

With this CI/CD setup, each iteration of the product can be validated and delivered swiftly, which is essential given the broad range of features (we want to catch regressions early – e.g., a change in tracking engine should not break the GUI’s reading of data, etc.). Using GitHub Actions’ Windows runners ensures our code is tested in a Windows environment similar to our target user environment.

### **7. User Onboarding & UX Best Practices**

Despite being a powerful tool, Due Time should feel simple for users, especially during onboarding. We will focus on **onboarding, user guidance, and usability testing** to polish the user experience:

* **Onboarding Flow:** The best onboarding is often a short and effortless one. When the user first installs and launches Due Time, we will do the following:

  1. **Initial Setup Prompt:** A quick welcome dialog explaining that tracking runs automatically. It will have a very brief wizard: ask for permission to auto-start on login (checkbox), and whether to enable AI features or keep them off initially (with a note that data won’t leave their PC unless enabled). We’ll default to off for AI for caution.
  2. **Create Default Projects:** Optionally, we might ask the user “Do you want to set up any projects now?” If the user is a freelancer, they likely have a couple of active projects/clients. We can let them enter a few names. If they skip, that’s fine – they can always add later. Having at least one project from the start can make the first day of data more meaningful (instead of everything unassigned).
  3. **Tooltips Tour:** Once the main dashboard opens, highlight key areas with subtle tooltips or overlays (e.g., “This is your timeline of activity. You can click on segments to assign them to projects.” and “Use the Projects tab to manage your client projects.”). This tour should be dismissible and not too verbose (3-4 tips at most). We may incorporate a “Learn more” link to a documentation page or FAQ for those who want details.

  The onboarding should take **< 2 minutes**. We want users to quickly get value (they’ll start seeing their time being tracked right away). As one UX best practice states, each additional required step can increase drop-off, so we keep it lean.

* **Ongoing Guidance:** Even after onboarding, the app should gently guide new users. For example, if after a day it sees a lot of unassigned time, it could show a non-intrusive notification or highlight: “You have 5 hours unassigned. Click here to categorize them and improve future suggestions.” This acts as a prompt to engage with the mapping tool, demonstrating the benefit. Similarly, if AI is off and the user is doing a lot of manual assignment, we could hint “Did you know? Due Time can suggest project tags for you using AI. Enable in settings.” (Of course, phrased carefully and not too frequently to avoid annoyance).

* **UX Best Practices Applied:**

  * **Non-intrusive operation:** The app runs mostly in the background. We avoid interrupting the user’s work. No pop-ups or alerts unless absolutely needed (like perhaps alerting if the app was quit and no tracking is happening, but even that could be just a tray icon change).
  * **Consistency:** Follow platform UI standards (e.g., use standard controls for dropdowns, date pickers, etc., so it feels consistent with other Windows apps). Use clear iconography (we might use Fluent UI System Icons for common icons, which align with Windows 10/11 style).
  * **Short, clear text:** All user-facing text (labels, tooltips, notifications) should be concise and easily understandable. For instance, instead of technical terms like “Set foreground window hook failed”, we show user-friendly messages (“Unable to start tracking. Please restart the app or contact support.”) if any error occurs.
  * **Undo/Confirmation:** Provide undo options for destructive actions. If a user deletes a project or a time entry, an “Undo” should be possible (or a confirmation dialog to avoid accidental loss). Because time data can be important, we double-guard deletion.
  * **User Empowerment:** Let the user feel in control. They can pause tracking if they want privacy (maybe they’re doing something personal and don’t want it logged at all – our pause feature allows that). They can edit anything tracked. Even AI suggestions are under their control to accept. This aligns with an important UX principle of **user autonomy**.

* **Usability Testing Plan:** We will conduct iterative usability tests:

  * **Internal Testing:** First, team members or friendly users test the app for a day or two, noting any confusing elements or bugs. We gather their feedback on the onboarding: did they understand how to assign projects? Could they find the reports easily?
  * **User Testing (Beta):** Recruit a small group of target users (5–10 freelancers/consultants) for a beta test. Provide them the app and maybe a short intro email. After one week of use, collect feedback via a survey or interview. Key questions: *Was it clear how to use the app? Which parts were confusing? Did it meet your needs? How was the performance?* Also ask about the AI suggestions if they tried them – *did the suggestions help or hinder?*
  * **Observation:** If possible, do a live observation (screen share or in-person) of a new user going through the onboarding. See where they hesitate or make errors. This can reveal UI elements that are not intuitive. For example, if we see a user struggling to figure out how to create a project rule, that indicates we need to simplify that flow or add a hint.
  * **Iterate:** Use the findings to tweak the UI. Maybe users couldn’t find the “Assign Project” button easily – we might then change it to a more prominent color or position. Or if they expected a feature that isn’t there (for example, a calendar view of their time), we note it for future but clarify the current usage.

* **Addressing Common UX Challenges:**

  * *What if the user forgets to use it?* – Since it’s automatic, the user doesn’t *need* to actively use it daily. But to derive value, they should review their data occasionally. We could send a weekly summary email (if user opts in) or a Windows Notification like “Your weekly productivity report is ready.” This can re-engage them. We must be careful with notifications – only meaningful ones, not spammy.
  * *Complexity of features:* We have advanced features (AI, extensive reports), but we should layer them progressively. A new user can use Due Time just as a simple logger and manually assign projects. As they get comfortable, they might explore AI suggestions or deeper analytics. We’ll ensure the UI doesn’t overwhelm by showing everything at once. Maybe hide AI-related UI until it’s enabled, etc.

* **Interface Examples from Competitors:** We looked at apps like ManicTime and RescueTime for inspiration:

  * ManicTime’s approach to tagging time is powerful but can be complex with multiple timelines and manual tagging. We want to simplify that: one combined view if possible, with automation to assist.
  * RescueTime focuses on productivity category (distracting vs productive). Our focus is project-based time tracking. We won’t emphasize “distracting time” categorization in MVP (though that could be an interesting feature later). This simplifies UI – no need to show a “productivity score” which RescueTime does. Instead, we focus on billable hours by project (since freelancers care about billing).
  * Clockify’s manual timer app is straightforward but requires user to start/stop timers. Our UI avoids timers altogether. However, we note Clockify’s design of listing time entries and editing them inline as something to emulate. Also, Clockify’s auto-tracker interface (which logs apps) is a simple table of apps with durations – that’s a baseline we surpass by linking to projects. But the simplicity of showing “App - Title - Duration - Idle%” can be borrowed for a certain view.

  &#x20;*Clockify’s Auto Tracker logs application usage locally (example): it lists each application/window used and duration, with idle time percentages. Due Time will present similar data, but enriched with project labels and allowing the user to merge or split entries as needed.*

* **Continuous UX Improvement:** Post-launch, we’ll keep a feedback channel (maybe an in-app “Send Feedback” that emails us or creates a GitHub issue). User feedback will drive refinements. We also plan to instrument the app with some **privacy-safe analytics**: e.g., count how many users enabled AI, or how often the project mapping feature is used. If we see a feature is rarely used, maybe it’s hidden or not valuable; if something is heavily used or requested, we focus on that. We have to implement analytics carefully, given our privacy stance (perhaps just opt-in usage metrics or a periodic voluntary survey, rather than silent telemetry). For MVP, likely no telemetry without user consent, but we’ll rely on direct feedback.

In short, our UX goal is to make Due Time a **“install and forget (in a good way)”** tool – it quietly does its job, and whenever the user interacts with it, it feels intuitive and helpful. By following onboarding best practices and testing with real users, we aim to achieve a polished experience.

## **Competitor Analysis and Go-to-Market Strategy**

The automatic time-tracking space has several established players and emerging tools. Understanding their offerings helps us position Due Time uniquely. Below we analyze key competitors – **RescueTime**, **ManicTime**, **Clockify**, and **Clockk** – and then outline our go-to-market and monetization strategy in light of this landscape.

### **Competitor Overview:**

* **RescueTime:** A pioneer in automatic time tracking for personal productivity. RescueTime runs in the background and categorizes time spent on applications and websites as “Productive”, “Distracting”, etc. It recently introduced a **Projects & Timesheets** feature with AI suggestions, moving closer to our target use-case (billing-oriented tracking). RescueTime’s strengths are its **cloud-based reports** (accessible via web or mobile) and focus on habit improvement (it can block distracting sites, set goals, etc.). It offers a subscription model (around \$6.50/month for individuals). One limitation is that detailed data is stored on their cloud, which might concern privacy-conscious users. Also, RescueTime historically was more about productivity scoring than granular project billing – the new timesheets feature is closing that gap by allowing project-specific tracking. Our takeaway: Due Time can differentiate by **local data ownership** and a tighter focus on freelancers’ billing needs. We’ll also provide a native Windows experience, whereas RescueTime’s interface is web-based for reports. RescueTime’s use of AI (OpenAI for project suggestions) validates our plan to do similar. We should strive to match their convenience (like drag-drop timeline editing) in the long run, but in MVP we can start simpler.

* **ManicTime:** A well-established automatic tracker, **primarily offline**. ManicTime records application usage and presents it in a rich client interface with multiple timelines (activity, documents, tags). It allows manual tagging of time segments to projects/tasks after the fact. ManicTime’s USP is that it can be used entirely offline (they also have a paid cloud for syncing multiple devices or teams, but single-user doesn’t require it). It’s very popular with freelancers and even team managers who want accurate timesheets without using cloud services. **Strengths:** highly detailed data (down to document names, as we noted), robust reporting, and a one-time purchase option (ManicTime offers a free tier with limited features and a Pro license for \~\$67 one-time, or a subscription for the cloud version) – many appreciate not having a subscription. It also has advanced features like pop-up reminders to tag away time, screenshot capture, and integrations (Jira, GitHub, etc.). **Weaknesses:** The UI, while powerful, can be overwhelming for new users. It’s Windows-only (the Mac version is far behind). Also, it lacks any AI assistance – all tagging is manual or rule-based (called “auto-tagging” rules, which users define themselves). Privacy-wise it’s strong (data stays local if you want). Our plan relative to ManicTime: We aim for a more modern UI (Fluent design) and introduce AI to reduce the manual tagging burden (“90% done for you” automation). We share the **local-first** approach with them, which is good. We can also compete on **simplicity** – highlight that Due Time is easier to use out-of-the-box, whereas ManicTime, while powerful, requires more user input to categorize data. Also, we will emphasize low resource usage – ManicTime is fairly optimized, but being older, a .NET 6 WPF implementation might use even less CPU (we’ll see).

* **Clockify:** Known mostly as a free web-based time tracker for teams, Clockify’s main use case is manual time entry (start/stop timers, assign to projects, then generate timesheet reports). It’s not automatic by default. However, Clockify has a **Desktop app with an “Auto Tracker” feature** that logs application usage locally. This auto-tracker is essentially a helper to let you fill your timesheet – it doesn’t automatically categorize by project; the user still has to click and convert those records into time entries. Notably, Clockify’s auto-tracker, like ours, keeps the data local and doesn’t send it to their servers until the user deliberately converts it to a timesheet entry. They also incorporate idle detection (with the ability to discard idle time from a running timer). Clockify’s huge advantage is being **free for unlimited users** in its basic version, which fueled its widespread adoption in teams and businesses. They monetize via paid features (like invoicing, admin controls). For an individual freelancer, Clockify is attractive because of cost and simplicity, but the downside is you must remember to start/stop timers or fill in entries manually (unless you use the auto tracker, which still needs manual confirmation). Also, Clockify being a web app means if you want detailed analyses or exports, you use their web interface. With Due Time, we pitch to the same users: “Clockify without the manual work.” Essentially, we combine Clockify’s **timesheet and project focus** with RescueTime/ManicTime’s **automatic tracking**. One specific pain point of Clockify’s auto-tracker that we address: *Clockify doesn’t distinguish different projects within the same app* (e.g., two Excel files for different clients just show up as “Excel” time). Due Time, via rules or AI, will separate those, which is a major benefit for freelancers juggling multiple projects in the same software.

* **Clockk:** A newer entrant (launched around 2020s) explicitly targeting freelancers with automatic time tracking. Clockk is cloud-based and advertises as **“AI powered”** to recognize projects. Their marketing claims Clockk “separates different projects done in the same app”, using AI to achieve this. They have a web interface where you can see your activities grouped by project after their AI processes it. Clockk’s focus is very aligned with what we’re building: no timers, just work and later allocate time to projects with AI assistance. They also emphasize privacy in that “everything tracked is private for your eyes only” (though the data does go to their cloud, they mean they’re not sharing it with others). The existence of Clockk validates our product idea strongly – there is demand for this among freelancers. However, Clockk is a paid service (as of 2023, around \$20/month after a trial, based on their site info). Also, since it’s cloud, one might have concerns similar to RescueTime. Our edge can be: a **one-time purchase or cheaper subscription**, local processing, and possibly a more responsive native app (Clockk’s UI is web-based which sometimes lags in user feedback compared to a desktop app). We should watch Clockk’s features – e.g., if they offer invoicing or integrations, we might consider those down the line to compete. But for MVP, just being an **offline-capable alternative** at a lower cost could attract users who balk at a monthly fee.

* **Others:** There are other manual trackers like Toggl Track, Harvest, etc., but those rely on timers (not automatic). There’s also open-source ActivityWatch, which is similar to ManicTime in concept (logs data locally, with a web UI) and fully free – but it’s quite technical and not user-friendly out of the box. We can note that advanced users might use ActivityWatch if they only trust open source, but for a mainstream freelancer, our polish and features will likely be more appealing than DIY solutions.

**Competitor Feature Matrix (abridged):**

| Tool               | Automatic Tracking                         | Project-based Categorization    | AI Assistance                        | Data Storage                            | Pricing Model                                                                |
| ------------------ | ------------------------------------------ | ------------------------------- | ------------------------------------ | --------------------------------------- | ---------------------------------------------------------------------------- |
| RescueTime         | Yes (apps/sites, focus % scores)           | Basic (new Timesheets feature)  | Yes (OpenAI suggestions)             | Cloud (with local client)               | Subscription (\$6.5/mo individual)                                           |
| ManicTime          | Yes (apps/sites/docs)                      | Yes (manual tagging & rules)    | No (manual only)                     | Local (optional cloud sync)             | Free (Basic) or License (\~\$67)                                             |
| Clockify           | Partially (Desktop auto-tracker logs apps) | Yes (but via manual timers)     | No (auto-tracker is rules-based)     | Cloud for main data, local for auto-log | Free (core); Paid add-ons                                                    |
| Clockk             | Yes (full auto, cloud)                     | Yes (auto-grouped by project)   | Yes (proprietary AI)                 | Cloud                                   | Subscription (\~\$20/mo)                                                     |
| **Due Time (MVP)** | Yes (full auto: windows/apps)              | Yes (manual rules + AI tagging) | Yes (OpenAI for tagging & summaries) | Local (SQLite; user-controlled backup)  | **To be determined** (Likely free trial & one-time or low-cost subscription) |

From this analysis, **Due Time’s unique selling points** are: a **privacy-centric, local-first approach** combined with modern AI assistance for categorization, all packaged in a **performance-efficient, native Windows app**. We’re essentially blending the best of ManicTime (local, detailed) with the best of Clockk/RescueTime (AI and ease) and targeting a price point or model that is attractive to solo users.

### **Go-to-Market Strategy:**

To successfully launch Due Time and reach our target users (freelancers, consultants, self-employed knowledge workers), we will follow a multi-pronged go-to-market approach:

* **Target Audience and Positioning:** Our primary users are individuals who need to track their work hours for clients or personal productivity, but who find manual timers annoying and have concerns about data privacy. This includes freelance designers, developers, writers, consultants, lawyers billing by hour, etc. We position Due Time as **“Your personal automatic timesheet assistant”** – emphasizing that it saves time (no manual tracking) and keeps their data private and under control (unlike some SaaS tools). For those coming from manual tools: “Due Time ensures you never forget to bill a minute by automatically recording your work.” For those coming from RescueTime: “Due Time gives you similar insights but focused on projects and without sending your data to the cloud.” For ManicTime users: “Due Time brings AI intelligence to automatic tracking, reducing the time you spend tagging.”

* **Pricing & Monetization:** We want to maximize adoption among individuals, so the strategy might be a **freemium model** or a modest one-time fee:

  * One approach: **Free Core, Paid Pro.** The core app (automatic tracking, manual project assignment, basic reports) is free to use locally. Advanced features like AI suggestions and summary reports could be part of a “Pro” upgrade (subscription maybe \$5-10/month or \$50/year, or even one-time if feasible). This way, users can try it out with no barrier and only pay if they want the fancy AI and convenience features. Given that AI API calls cost us money, putting AI behind a subscription makes sense to cover those costs.
  * Another approach: **30-day Free Trial then Paid** (like ManicTime does). We could offer the full functionality free for a month, then require purchase for continued use. However, purely paid up front might limit user uptake in a world where Clockify is free. Freemium might be better to build a user base.
  * **One-time vs Subscription:** Many freelancers might prefer a one-time purchase (like they buy a tool and own it, as with ManicTime). We could offer both options: e.g., \$X one-time for a perpetual license (with 1 year of updates) or a cheaper subscription for always-updated. ManicTime’s model of one-year updates then optional renewal is a proven approach we could emulate. It captures upfront value and encourages renewals for major upgrades.
  * Initially, we might launch as **free beta**, gather users and feedback. Once the product is stable and valuable, introduce paid plans. Early adopters could get a discount or grandfathered pricing as goodwill.

* **Distribution Channels:**

  * **Website & SEO:** We’ll create a landing page (duetimeapp.com, for example) that clearly communicates the value proposition. It will have call-to-action to download (for Windows) and highlight features (especially “automatic”, “AI”, “privacy”). We’ll include a comparison section vs other tools to help SEO (like blog posts or pages “ManicTime vs Due Time” etc. which people search for alternatives). SEO content marketing could include articles about “how to track time automatically”, “time tracking tips for freelancers”, linking to our solution.
  * **Communities:** We will announce and discuss Due Time in relevant communities: for instance, the /r/freelance or /r/productivity or /r/software communities on Reddit, or Hacker News (the tech crowd might appreciate the local-first, privacy angle). Also, communities like Indie Hackers or Designer News for solo professionals. We should be transparent, not spammy – perhaps sharing a blog about our development journey and inviting feedback.
  * **Product Hunt Launch:** Product Hunt is a great platform to launch new productivity tools. We can prepare a PH launch with a polished description and media (screenshots, maybe a short demo video) to catch the early adopters and tech enthusiasts. A successful PH launch can generate initial user signups and press inquiries.
  * **Freelancer Platforms & Forums:** Consider advertising or sharing on platforms like Upwork, Freelancer.com forums, or specialized freelance communities (maybe forums for consultants, etc.). The message would be along the lines of “track your billable hours effortlessly.” Perhaps even partnering with freelance tool blogs or newsletters for a review.
  * **Microsoft Store:** We should consider publishing Due Time on the Microsoft Store (since it’s a Windows app). This can increase visibility as users search the Store for “time tracker”. The MS Store allows free or paid apps; we could list a free version there. Even if the store isn’t a primary channel, being there adds credibility and ease of install (one-click for Windows 10/11 users).
  * **Word of Mouth & Referrals:** If we nail the experience, users will recommend it to peers. We might include a subtle “Refer a friend” or just rely on organic sharing. If we do referrals, maybe offer a free month of Pro for both the referrer and friend to encourage spreading the word.

* **Marketing Messaging:** Emphasize:

  * “**No Timers, No Fuss:** Due Time automatically tracks your work so you can focus on your tasks, not on the timer.” (This appeals to those who dislike the disruption of manual tracking).
  * “**Private & Secure:** Your data never leaves your PC unless you want it to. We respect your privacy – no big brother, just you and your productivity.” (This differentiates from cloud tools and appeals to privacy-aware users).
  * “**Smart Insights:** Leverage AI to categorize work and get weekly summaries. It’s like having a personal assistant prepare your timesheet and report.” (This highlights the innovative aspect and time-saving nature).
  * Testimonials can be powerful. Once we have happy beta users, include quotes like *“I recovered 5 hours of billable time in the first week that I’d have otherwise forgotten. Due Time paid for itself immediately!”* (Hypothetical example).

* **Monetization beyond software:** In the future, we could consider value-added services: e.g., a cloud sync service (run your own server or our hosted) for multi-device use, team features for small agencies (like a team lead can see combined reports – this enters employee monitoring territory, but perhaps only aggregated by project). We could charge teams per user monthly. Another angle is integrating with billing/invoicing – e.g., generate an invoice for a client from tracked hours – possibly via integration with tools like QuickBooks or FreshBooks. Those could be premium features or separate modules. For MVP though, we keep scope limited, but keep the business model in mind that expanding to these areas can provide additional revenue streams.

* **Competition Response:** We should be prepared that competitors might react (for example, ManicTime could start adding AI features too, or Clockify could improve their auto-tracker). Our defense is building a loyal user base and continuously innovating. We plan to be relatively open (maybe not fully open-source initially, but could consider open-sourcing parts to build community trust). If we focus on a niche (freelancers who care about privacy), even big players might not serve them perfectly, leaving us space. The market is large (millions of freelancers worldwide), so carving our niche is feasible with the right approach.

* **KPIs for Launch Success:** We will measure:

  * Number of downloads in first 3 months.
  * Conversion rate to active users (tracking how many continue using beyond 1 week). We could measure active usage by updates checks or if we add telemetry (opt-in). Alternatively, track how many come back for updates or join our community.
  * User feedback volume and sentiment. If we get glowing reviews and low churn, that’s a success indicator.
  * Eventually, conversion to paid (if we have freemium, what % upgrade). But initially focus on user base and satisfaction.

By carefully analyzing competitors, we’ve ensured Due Time’s feature set and positioning hit a sweet spot that isn’t fully occupied: **automatic, intelligent, and privacy-centric time tracking for individuals**. Our go-to-market will leverage that uniqueness to attract users who are looking for exactly this combination.

## **Development Roadmap**

To implement this project in an organized way, we break the work into phases. Each phase has a clear objective, key tasks, and success criteria to know when it’s complete. This ensures we can track progress and maintain focus on MVP features first, then iterate. Below is the **Phase → Objective → Key Tasks → Success Criteria** table:

| **Phase**                          | **Objective**                                                         | **Key Tasks**                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | **Success Criteria**                                                                                                                                                                                                                                                                                                                        |
| ---------------------------------- | --------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **1. Planning & Design**           | Define the project scope, requirements, and technical approach.       | - Requirements gathering and competitor research (✅ done).<br>- Choose tech stack (WPF, SQLite, OpenAI) and design architecture.<br>- Create detailed specifications (this document).                                                                                                                                                                                                                                                                                              | ✅ **Design Approved:** All stakeholders sign off on the MVP plan (feature list, tech choices). Team has a clear blueprint to follow.                                                                                                                                                                                                        |
| **2. Core Tracking Engine**        | Develop the background service to log time data accurately.           | - Set up project structure (solution, projects for core and app).<br>- Implement foreground window capture (WinEventHook) and idle detection (GetLastInputInfo).<br>- Create SQLite DB schema and data access layer (using an ORM or raw queries).<br>- Write unit tests for time logging logic (simulate window switches & idle).                                                                                                                                                 | ✅ **Engine Ready:** The service can run (even via a console for testing) and log window changes and idle periods to the SQLite DB reliably. Test: logs show correct sequence of events when switching apps. CPU overhead minimal in tests.                                                                                                  |
| **3. Basic GUI & Project Mapping** | Provide a user interface to view tracked data and assign projects.    | - Create WPF UI project with Fluent styling (navigation, main window, basic pages: Dashboard, Projects, Settings).<br>- Implement display of today’s tracked entries from DB (simple list or basic timeline control).<br>- Implement adding/editing Projects and mapping rules UI.<br>- Enable manual assignment: user can select an entry and assign it to a project (updates DB).                                                                                                | ✅ **GUI Usable:** User can see a list of time entries populating in the app from the service. They can create a project and assign an entry to it, and see the change reflected. The UI doesn’t need to be final polished, but is functional and not confusing.                                                                             |
| **4. AI Feature Integration**      | Introduce AI for auto-tag suggestions and summary generation.         | - Integrate OpenAI API (create an API client, manage API keys securely in config).<br>- Implement classification logic: function that takes an unassigned entry + project list, returns suggested project. Use a mock first, then live API.<br>- UI indication for AI suggestions (e.g., suggested label next to entries).<br>- Implement weekly summary generation: aggregate data and call OpenAI to summarize.<br>- Provide UI to trigger summary (e.g., “View Weekly Report”). | ✅ **AI Working:** In a test scenario, the app suggests a project for a new entry that matches a known keyword (e.g., window title “ClientX” -> suggests Project ClientX). Summaries produce coherent text covering the week’s main points. (We validate with a few manual examples.) Users can toggle AI features off without breakage.     |
| **5. Polish & Performance Tuning** | Refine the UI/UX, improve performance, and fix bugs based on testing. | - Apply Fluent Design touches (icons, colors, dark mode toggle).<br>- Ensure responsiveness (large data sets handling, virtualization in lists if needed).<br>- Profile CPU and memory while simulating a full day’s use; optimize any hot spots (e.g., reduce DB writes, optimize queries with indexes).<br>- Improve installer (create MSI or setup.exe), implement auto-run on login setting.<br>- Conduct internal testing and gather feedback, fix any usability issues.      | ✅ **MVP Complete:** The app looks professional and runs within targets (e.g., <3% CPU, \~50MB RAM in idle state). Testing confirms stability (no crashes in 1+ day of continuous running). All major bugs from testing are resolved. The installer installs the service and app correctly and the app auto-starts as expected.              |
| **6. Beta Launch & Feedback**      | Release the MVP to a closed group and iterate.                        | - Package the MVP as Beta version.<br>- Provide it to selected beta users (or via Product Hunt launch if open beta).<br>- Support users in setup, collect feedback and bug reports (via email or tracker).<br>- Triage feedback: fix critical bugs immediately (issue quick updates), note feature requests for future.                                                                                                                                                            | ✅ **Beta Validated:** At least 5–10 users have used the app for 1-2 weeks and provided feedback. No showstopper bugs remain (the app reliably tracks time on various setups). Feedback indicates the core value is delivered (e.g., testimonials like “this saved me time”). We have a backlog of improvements based on real user input.    |
| **7. Public Release & GTM**        | Prepare for wider release and implement go-to-market steps.           | - Set up website and documentation (user guide, FAQs).<br>- Implement any minor improvements identified as critical before public launch (e.g., better onboarding prompt, additional export option, etc.).<br>- Ensure licensing/activation mechanism if it’s a paid product (or implement trial limits if needed).<br>- Launch on website, announce on forums/PH as planned.                                                                                                      | ✅ **Launch Success:** The product is released publicly with the website live. Initial downloads/users are coming in (some metric of engagement). We monitor for any post-launch issues (via support channels) and are ready to release small patches. Monetization (if active) starts to trickle in, or we gather interest for conversions. |

*(The above timeline assumes a roughly 3-4 month schedule for MVP through Beta given a small team, but can be adjusted. Phase 1 (planning) is already done by delivering this document. Phase 2 and 3 are core development, possibly done in parallel by different team members (one on engine, one on GUI) with integration at end of Phase 3. Phase 4 (AI) might overlap with 3 or come after basic UI. Phase 5 is a buffer for polish and making sure we hit quality benchmarks. Phase 6 and 7 involve part development (fixes) and part marketing.)*

This structured plan ensures we build the right things in the right order – core functionality first, then “nice-to-have” features like AI and visual polish, culminating in a tested, market-ready product.

## **Future Improvements and Stretch Goals**

While the MVP focuses on a single-user Windows application, we have identified several stretch goals and future enhancements that can be addressed after the MVP or in a scaled-up version of the product:

* **Team and Scalability:** In the future, Due Time could be expanded for small teams or organizations. This would involve developing a central server or cloud service where multiple users’ data can aggregate. For example, a team manager could see a summary of the team’s time allocation (with proper privacy controls). We’d need user accounts, syncing from client apps to server, and admin controls. This is a significant expansion (essentially creating a SaaS), so it’s beyond MVP. However, our architecture can be prepared by abstracting the data layer – e.g., if we have a repository interface for time entries, we can implement a cloud-backed repository later while reusing most of the UI and tracking logic. Ensuring our data model includes a `user_id` or similar (even if just one user now) would make multi-user data easier later. Scalability also means performance with large data: we should test with, say, 2 years of data (100k entries) to ensure the app can handle it (SQLite can, if indexed well). If not, we might consider archiving older data or moving to a more robust database when scaling to org-level usage.

* **Internationalization (i18n):** Currently, we plan the UI in English. But freelancers around the world could benefit from this tool. We should internationalize the UI strings by using resource files in WPF, so that adding new languages is just a matter of translating those resources. Also consider locale differences: week start day (Monday vs Sunday), date/time formats (24h vs 12h clock), maybe even different work week structures. Time zone handling if we ever have data syncing (for now all on one PC so local time is fine). We’ll design with Unicode and cultural formats in mind from the start (using .NET’s culture for date display, etc.). For example, if we eventually localize to Spanish, the UI and summary text should appear appropriately. We might not do translation in MVP, but having the framework ready saves effort later.

* **Integrations and Extensibility:** We can make Due Time more powerful by integrating with other tools:

  * *Calendar Integration:* e.g., fetch meetings from Outlook or Google Calendar to mark those time blocks or to exclude them from idle (RescueTime’s timeline does this with calendar events). We could then auto-tag meeting times as such or compare scheduled vs actual time.
  * *Task/Project Management:* If a user uses Jira, Asana, Trello, etc., perhaps allow linking a project to one of those and automatically include references (RescueTime’s highlights include “updates made to integration tasks like Asana, Trello, GitHub”). E.g., if commit messages or branch names contain a task ID, we could tag time to that task. This is advanced and likely post-MVP, but having an architecture that can accept plugin or integration modules is useful.
  * *API for Developers:* Provide a local REST API or SDK so users can query their own data or build custom reports. ActivityWatch (open source tracker) does this via a local web API. In Due Time, we could have a toggle to enable an HTTP API that returns JSON of time entries, etc., allowing tech-savvy users to do more with their data (could be a differentiator for developer audience).

* **Performance Testing & Optimization:** Once the app is in use, we’ll gather real performance data. We should set up automated performance tests: e.g., simulate a heavy usage scenario (8 hours with switching every minute, lots of entries) and measure memory, or open the app UI with 1 month of data loaded. We can use profiling tools (Visual Studio Profiler, dotMemory, etc.) to find any memory leaks or slow functions. For instance, ensure that the idle check thread doesn’t inadvertently cause CPU spikes, or the DB doesn’t lock up if writing and reading concurrently. The target of <3% CPU is arbitrary but we should test on a typical machine (say a mid-range laptop) by having it run and do other tasks, measure CPU usage via Windows Performance Monitor. If we see any deviations, optimize (maybe reduce event frequency, optimize string handling, etc.). As part of stretch, we might implement more native code for heavy tasks if needed (though likely not needed).

* **UI Enhancements:** After MVP, there are many possible UI improvements:

  * A full timeline view with drag-and-drop editing (like a gantt chart of your day). This makes adjusting entries easier. We deferred it for MVP due to complexity, but it’s high on a power-user’s wish list.
  * Dashboard charts: e.g., pie chart of time by project for the week, bar chart of hours per day. Visual summaries can complement the text summary. Using a chart library or WPF’s drawing can implement this.
  * Notifications and Reminders: e.g., a reminder at end of day: “You have 1h unassigned. Assign now?” with a quick action. Or weekly “Check your report”.
  * Mobile Companion App: A read-only mobile app (Android/iOS) where the user can see their time data on the go or start/stop tracking manually for offline tasks. This ties into sync though.
  * Multi-monitor/window tracking: see which monitor had focus or track if multiple windows side by side (this is advanced—some trackers log all open windows not just active, but that’s a lot of data).

* **Post-Prototype Pilot Testing:** Once MVP is successful, we’d likely do a larger pilot with maybe 50-100 users (perhaps via an open beta or through a partnership with a freelancer community). The goal is to observe usage in the wild at scale, get feedback on edge cases, and ensure reliability before calling it a 1.0 product. We’ll set up a bug tracker or forum for these pilot users. We might also instrument the app (with permission) to collect anonymized stats like crashes or exceptions. Pilot testing can also help refine our onboarding and documentation because we’ll see how different types of users approach the app.

* **Potential for Machine Learning on Device:** Long term, to further differentiate and address privacy, we could train local ML models on the user’s data for classification, so dependency on cloud AI is reduced. For instance, an on-device model using ML.NET could classify window titles into projects after it’s seen enough examples from the user. This could be a power feature for those who don’t want any cloud usage but still want AI-like help. It’s a challenging but exciting direction (would require collecting a lot of labeled data per user or bootstrapping from a generic model).

* **Open Source Consideration:** We might consider open-sourcing parts of Due Time (perhaps the core tracking engine) to build trust and allow community contributions (especially for platform-specific hooks or integration with other OS if we go cross-platform). Keeping sensitive parts (like proprietary UI or business logic) closed might be an option while open-sourcing the fundamental tracking. This strategy can sometimes accelerate development (enthusiasts may contribute features or language translations). It also aligns with transparency for a privacy tool. It’s a business decision – if monetization is mainly services or AI, open-sourcing core might be okay. Not immediate, but a thought for the future.

By acknowledging these stretch goals now, we ensure our MVP design is forward-compatible. We won’t implement these features yet, but our code should be structured to allow adding them. For example, knowing that team sync might come, we keep the data model flexible; knowing that cross-platform may come, we don’t embed Windows-only assumptions in core logic (only in the tracking layer which can be swapped per OS). This future-thinking safeguards us from cornering the architecture in a way that makes expansion hard.

---

With the above comprehensive plan, Due Time is poised to deliver a powerful yet user-friendly MVP that meets the needs of freelancers and knowledge workers. By focusing on core values – **automation, intelligence, performance, and privacy** – and layering on thoughtful design and future-ready architecture, we aim to build not just an application, but a productivity partner that can grow and adapt with user needs. The next step is to execute this plan, iterate with user feedback, and make “no more manual timers” a reality for our users.
